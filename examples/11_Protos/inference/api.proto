// Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

syntax = "proto3";

package nvidia.inferenceserver;

// Request header for inferencing. The actual input data is delivered
// separate from the header.
message InferRequestHeader {
  // Input...
  message Input {
    // Name of the input.
    string name = 1;

    // Size of the input, in bytes. This is the size for one instance
    // of the input, not the entire size of a batch of the input.
    uint64 byte_size = 2;
  }

  // Output...
  message Output {
    // Name of the output.
    string name = 1;

    // Size of the output, in bytes. This is the size for one instance
    // of the output, not the entire size of a batch of the output.
    uint64 byte_size = 2;

    // Class result format. The output must be a vector. Output values
    // will be interpreted as probabilities and the highest 'count'
    // values will be returned.
    message Class {
      // Return the 'count' highest valued results.
      uint32 count = 1;
    }

    // Optional. If defined return this result as a classification
    // instead of raw data.
    Class cls = 3;
  }

  // Batch size of the inference inputs.
  uint32 batch_size = 1;

  // Inference inputs.
  repeated Input input = 2;

  // Inference outputs that are being requested.
  repeated Output output = 3;
}

// Response header for inferencing. Any raw response data (i.e. tensor
// values) is delivered separately from the header.
message InferResponseHeader {
  // Output...
  message Output {
    // Name of the output.
    string name = 1;

    // Raw result
    message Raw {
      // Size of the output, in bytes. This is the size for one
      // instance of the output, not the entire size of a batch of the
      // output.
      uint64 byte_size = 1;
    }

    // Classification result
    message Class {
      // The index in the output tensor.
      int32 idx = 1;
      // The value of the class as a float (typically a probability).
      float value = 2;
      // The label for the class (optional, only available if provided
      // by the model).
      string label = 3;
    }
    message Classes {
      // The topk classes for this output
      repeated Class cls = 1;
    }

    // Result format for this output. Only one of these may be
    // specified. For 'batch_classes' there should be one entry for
    // each output of the batch.
    Raw raw = 2;
    repeated Classes batch_classes = 3;
  }

  // Name of the model that produced the results.
  string model_name = 1;

  // Version of the model that produced the results.
  uint32 model_version = 2;

  // Batch size of the inference outputs.
  uint32 batch_size = 3;

  // The outputs
  repeated Output output = 4;
}
