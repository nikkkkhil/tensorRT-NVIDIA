// Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
// Copyright (c) 2018-2019, TensorFlow Authors. All rights reserved.

syntax = "proto3";

package nvidia.inferenceserver;

// Data types supported for input and output tensors.
enum DataType {
  TYPE_INVALID = 0;

  TYPE_BOOL = 1;

  TYPE_UINT8 = 2;
  TYPE_UINT16 = 3;
  TYPE_UINT32 = 4;
  TYPE_UINT64 = 5;

  TYPE_INT8 = 6;
  TYPE_INT16 = 7;
  TYPE_INT32 = 8;
  TYPE_INT64 = 9;

  TYPE_FP16 = 10;
  TYPE_FP32 = 11;
  TYPE_FP64 = 12;
}

// A group of one or more instances of a model and resources made
// available for those instances.
message ModelInstanceGroup {
  // Kind of this instance group.
  enum Kind {
    // This instance group represents instances that can run on either
    // CPU or GPU. If all GPUs listed in 'gpus' are available then
    // instances will be create on GPU(s), otherwise instances will be
    // created on CPU.
    KIND_AUTO = 0;

    // This instance group represents instances that must run on the
    // GPU.
    KIND_GPU = 1;

    // This instance group represents instances that must run on the
    // CPU.
    KIND_CPU = 2;
  }

  // Optional name of this group of instances. If not specified the
  // name will be formed as <model name>_<group number>. The name of
  // individual instances will be further formed by a unique instance
  // number and GPU index:
  //   <name>_<instance number>_gpu<gpu index>
  string name = 1;

  // The kind of this instance group. Default is KIND_AUTO. If
  // KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
  // may be specified. If KIND_CPU only 'count' is valid and 'gpu'
  // cannot be specified.
  Kind kind = 4;

  // Number of instances in this group created for each GPU listed in
  // 'gpus'. Default is 1.
  int32 count = 2;

  // GPU(s) where instances should be available. For each GPU listed,
  // 'count' instances of the model will be available. Setting 'gpus'
  // to empty (or not specifying at all) is eqivalent to listing all
  // system GPUs.
  repeated int32 gpus = 3;
}

// Input tensor for the model
message ModelInput {
  // Format for the input.
  enum Format {
    // The input has no specific format.
    FORMAT_NONE = 0;

    // Image formats. Tensors with this format require 3 dimensions if
    // the model does not support batching (max_batch_size = 0) or 4
    // dimensions if the model does support batching (max_batch_size
    // >= 1). In either case the 'dims' below should only specify the
    // 3 non-batch dimensions (i.e. HWC or CHW).
    FORMAT_NHWC = 1;
    FORMAT_NCHW = 2;
  }

  string name = 1;
  DataType data_type = 2;
  Format format = 3;
  repeated int64 dims = 4;
}

// Output tensor for the model
message ModelOutput {
  string name = 1;
  DataType data_type = 2;
  repeated int64 dims = 3;

  // Label file for this output (optional).
  string label_filename = 4;
}

// Policy indicating which versions of a model should be made
// available by the inference server.
message ModelVersionPolicy {
  // Serve only the 'num_versions' highest-numbered versions.  This is
  // the default policy and the default value of 'num_versions' is 1,
  // indicating that by default only the highest-number version of a
  // model will be served.
  message Latest {
    uint32 num_versions = 1;
  }

  // Serve all versions of the model.
  message All {
  }

  // Serve only a specific set of versions of the model.
  message Specific {
    repeated int64 versions = 1;
  }

  // Each model must implement only a single policy. The default
  // policy is 'Latest'.
  oneof policy_choice {
    Latest latest = 1;
    All all = 2;
    Specific specific = 3;
  }
}

// Model configuration.
message ModelConfig {
  // Name of the model.
  string name = 1;

  // Type of model (e.g. "tensorflow").
  string platform = 2;

  // Policy indicating which version(s) of the model will be served.
  ModelVersionPolicy version_policy = 3;

  // Maximum batch size allowed for inference. This can only decrease
  // what is allowed by the model itself. A value of 0 indicates that
  // batching is not-allowed/is-disabled (for some input formats this
  // has implications on the expected dimension of the inputs, see
  // Format above).
  int32 max_batch_size = 4;

  // Inputs and outputs to the model.
  repeated ModelInput input = 5;
  repeated ModelOutput output = 6;

  // Optional instances of this model. If not specified, one instance
  // of the model will be instantiated on each available GPU.
  repeated ModelInstanceGroup instance_group = 7;

  // Optional filename of the model file to use if a
  // compute-capability specific model is not specified in
  // 'cc_model_names'. If not specified the default is model.graphdef
  // for TF graphdef models and model.plan for TensorRT PLAN models.
  string default_model_filename = 8;

  // Optional map from CUDA compute capabilities to the filename of
  // the model that supports that compute capability. The filename
  // refers to a file within the model version directory.
  map<string, string> cc_model_filenames = 9;
 }

// List of model configurations.
message ModelConfigList {
  repeated ModelConfig config = 1;
}
